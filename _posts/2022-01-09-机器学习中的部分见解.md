---
layout:     post
title:      "机器学习中的部分见解"
subtitle:   "ideas about Machine-learning"
date:       2021-01-09 12:00:00
author:     "lyp"
header-style: text
tags:
    - machine_learning
    - CNN

---

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: {
      equationNumbers: {
        autoNumber: "AMS"
      }
    },
    SVG: {
      scale: 90
    },
    tex2jax: {
      inlineMath: [ ['$','$'] ],
      displayMath: [ ['$$','$$'] ],
      processEscapes: true,
    }
  });
</script>
<script type="text/javascript"      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG">
</script>

## 深度学习部分

### 	1. 从布尔逻辑角度分析神经网络的拟合能力和“深”的好处

#### 	1.1 直观理解

1. 单个感知机能模拟与/或/非门，但却不能表示异或运算。

   <img src="https://pic3.zhimg.com/80/v2-acca5a158cfb4e55124f6b2155a539be_720w.jpg" alt="img" style="zoom:65%" />

   同时针对多元输入仍旧可以模拟，设输入中正项个数为$L$,负项个数为$N$

   在与门中，将神经元的激活阈值设为$L$即可模拟与门；在或门中，将神经元的激活阈值设为$L-N+1$即可模拟或门，具体如下图所示：

   <div align='center'>
       <img src="https://gitee.com/cwkyaoyao/blog_img/raw/master/img/image-20200802105006696.png" alt="image-20200802105006696" style="zoom:50%;" />
       <img src="https://lypro.top/img/in-post/post-machine_learning/screenshot-www.cnblogs.com-2021.12.30-13_40_55.png" alt="image-20200802105033012" style="zoom:52%;" />
   </div>

2. 但将多个感知机组合起来，只关心输入和输出时，就可以表示异或运算

   <img src="https://gitee.com/cwkyaoyao/blog_img/raw/master/img/image-20200802123743467.png" alt="image-20200802123743467" style="zoom:55%;" />

   

3. 对于任意的布尔函数，都可以变成真值表，然后针对每一项转化为DNF，最后通过MLP实现

   <img src="https://gitee.com/cwkyaoyao/blog_img/raw/master/img/image-20200802105132208.png" alt="image-20200802105132208" style="zoom:60%;" />

#### 1.2 理论分析

> - 依据《图论》，一个有向图的“深度”是指，输入结点和输出结点间的最长路径长度因此，一个堆叠的网络结构，如果深度超过2，即定义为深度网络。
> - 多层感知机可以构造任意形式和组合的布尔运算，那么，表达一个通用的布尔运算函数，到底需要多少层和多少个神经元的感知机网络呢？

1. 一层感知机网络，即可表示通用布尔运算函数。我们可以利用穷举法，将任意的布尔函数变成真值表，然后针对每一项转化为DNF，最后通过MLP实现。但这样组成的网络不一定是最简洁的。

2. 我们将布尔函数转化为卡诺图进行化简，如图所示，这是一个四变量卡诺图，黄色为1，白色为0。按横/纵相邻的最大$2^n$个黄色1合并，可简化真值表DNF公式，因此可简化隐含层个数。

   <img src="https://pic4.zhimg.com/v2-fdd2fd5a9d95c0cefb9d5234203e0843_r.jpg" alt="preview" style="zoom:60%; " />

3. 但是在最坏情况下，一层感知机网络中神经元的最大数目$2^n$。即，白色方格的个数。神经元个数是变量数的指数倍。卡诺图中，任意真值1的方格均不能合并，DNF公式无法化简。如下图：左右分别为四变量卡诺图，六变量卡诺图（白色为1，红色为0）

   <img src="https://pic4.zhimg.com/v2-b8ec8e6b0f81a31f7e55eeb6603d578f_r.jpg" alt="preview" style="zoom:80%;" />

   

   4.最坏情况下，我们要使用异或法及其结合律增加层数，从而减少神经元个数

   <img src="https://pic3.zhimg.com/v2-92daf23414a36bd8ee0ea97150c628fa_r.jpg" alt="preview" style="zoom: 67%;" />

   > **MLP可以模拟任意的布尔函数，但是需要足够的宽度或者深度，最佳的宽度和深度取决于布尔函数的复杂度，有更多的层，可以大幅度降低网络的大小。同时，参数个数依赖神经元个数，如果神经元个数为指数级，必然要求指数级或超指数级的参数个数。**

### 2. 从分类的角度分析神经网络的拟合能力

1. 当输入和输出为实数时，如果加权和超过阈值，激活感知机。本质上，实数输入的感知机，将构成线性分类器。

2. 因此，当任意多个线性分类器适当组合后，可以表示任意决策面。

   <img src="https://pic4.zhimg.com/80/v2-b1931bdd3a9ec8589e17c534c2119343_720w.jpg" alt="img" style="zoom:67%;" />

   > 通过对多个决策边界的组合归并，多层感知机可以构造任意形状的分类超平面组合。既然一层网络可以拟合任意的布尔函数，那我们能否只用一层来拟合任意分类超平面呢

   3.**一层感知机网络：**无穷多个神经元，视精度而定，随着神经元越多，分割线会越多

   <img src="https://pic1.zhimg.com/v2-4746842325717cfe03ae576085628d88_r.jpg" alt="preview" style="zoom:80%;"/>

   4.总能通过不断的增加决策线，使中心接近圆柱体。中心值为N，并快速下降到N/2。注意，圆柱体本身可位于任何位置，视组合的决策线位置而定
   
   <img src="https://pic4.zhimg.com/v2-ee38972245672261ff5288bf153a03b3_r.jpg" alt="preview" style="zoom:63%;" />$\rightarrow$<img src="https://pic1.zhimg.com/v2-f3b8ab08d3976b87f657fd5af55c5bdc_r.jpg" alt="preview" style="zoom: 55%;" />
   
   5.我们将其无限填充就可拟合出任意决策超平面,虽然单层无限多个神经元的感知机网络可拟合任意决策面，**但深度网络只需更少的神经元就能达到目的。**
   
   <img src="https://pic2.zhimg.com/80/v2-1a879afccc4fcf259595e1db7631f679_720w.jpg" alt="img" style="zoom: 67%;" />

### 3. 从曲线拟合的角度分析神经网络的拟合能力

1. 在二维空间内，我们可以利用脉冲激活然后两两一组组成方形从而拟合各种函数

   <img src="https://gitee.com/cwkyaoyao/blog_img/raw/master/img/image-20200802135516239.png" alt="image-20200802135516239" style="zoom:60%;" />

2. 在更高维的空间内，可像上文描述的那样模拟圆柱体从而拟合不同的超平面

   <img src="https://gitee.com/cwkyaoyao/blog_img/raw/master/img/image-20200802135627669.png" alt="image-20200802135627669" style="zoom:60%;" />

   > 上述我们讨论的均为阈值激活函数，即达到阈值激活为固定值，达不到则为0，这样导致当网络容量不足（神经元个数）不足时，无法捕获到所有模式。

3. 替换为其他的激活函数

   相比阈值函数的非黑即白，其他激活函数提供了捕获模式中更多的分辨率信息，这些分辨率信息会以梯度的形式，从低层传递到高层，甚至可以补偿由低层丢失的其他信息。

   ![img](https://pic2.zhimg.com/80/v2-85c58ef25522195448c6514c328a6f8d_720w.jpg)

   越充分的梯度信息，能传递更多的信息。比如，relu优于更易饱和的sigmoid。但同时也要求更深的网络层数。

   理论上，只要有足够的深度和宽度，任何激活函数，甚至是阈值函数，都能捕获全部的模式信息。<br>如果使用阈值激活，一旦深度和宽度不够，丢失的模式将得不到任何弥补。但激活函数的选择，可以缓解该问题，选择一个好的激活函数，其梯度会在高层补偿因低层容量不够而丢失的信息。<br>越能持续捕获模式的激活函数，将越能补偿丢失的信息。因此，这也是为什么大家普遍使用relu来替换sigmoid和阈值函数，因为，在相同的宽度和深度下，它能捕获更多的信息。

### 4. 全连接神经网络的推理与反向传播

`项目源码`：[FC source-code](https://github.com/lyp2333/External-Attention-pytorch/blob/master/test_model/my_net/mynet.py)<a name='fc_code'> </a>

> 激活函数采用 `Sigmoid` 函数，误差函数为 `MSE` 损失函数的神经网络为例

1. 激活函数求导

   sigmoid函数表达式为$\sigma(x)=\frac{1}{1+e^{-x}}$,其导数为:$\sigma(x)-\sigma(x)^2=\sigma-\sigma^2$

2. 均方差误差梯度

   此时代价函数为$ C=\frac{1}{2}||y-a^L||^2=\frac{1}{2}\sum_j(y_j-a_j^L)^2 $

   计算最后一层神经网络产生的错误：$\delta^L=\nabla_aC\odot\sigma'(z^L)$

3. 由后往前计算每一层网络产生的误差
   $$
   \begin{aligned}
   \because \delta_j^l &= \frac{\partial{C}}{\partial{z_j^L}}
   		   = \sum_k{\delta_k^{l+1}\centerdot\frac{\partial(w_{kj}^{l+1}a_j^l+b_k^{l+1})}{\partial a_j^l}\centerdot\sigma'(z_j^l)}\\
   		   &= \sum_k{\delta_ k^{l+1}\centerdot w_{kj}^{l+1}\centerdot \sigma'(z_j^l)}\\
   \therefore	\sigma^l   &= ((w^{l+1})^T\delta^{l+1}\odot\sigma'(z^l))
   \end{aligned}
   $$

4. 计算权重的梯度

   $$
   \frac{\partial C}{\partial w_{j k}^{l}}=\frac{\partial C}{\partial z_{j}^{l}} \cdot \frac{\partial z_{j}^{l}}{\partial w_{j k}^{l}}=\delta_{j}^{l} \cdot \frac{\partial\left(w_{j k}^{l} a_{k}^{l-1}+b_{j}^{l}\right)}{\partial w_{j k}^{l}}=a_{k}^{l-1} \delta_{j}^{l}
   $$
   
5. 计算偏置的梯度
   $$
   \frac{\partial C}{\partial b_{j}^{l}}=\frac{\partial C}{\partial z_{j}^{l}} \cdot \frac{\partial z_{j}^{l}}{\partial b_{j}^{l}}=\delta_{j}^{l} \cdot \frac{\partial\left(w_{j k}^{l} a_{k}^{l-1}+b_{j}^{l}\right)}{\partial b_{j}^{l}}=\delta_{j}^{l}
   $$
   伪代码如下：

   - 输入训练集

   - 对于训练集中的每个样本x，设置输入层对应的激活值$a^1$

     1. 前向传播：$z^l=w^la^{l-1}+b^l,a^l=\sigma(z^l)$
     2. 计算输出层产生的误差：$\delta^L=\nabla_aC\odot\sigma'(z^L)$
     3. 反向传播误差：$\delta^{l}=\left(\left(w^{l+1}\right)^{T} \delta^{l+1}\right) \odot \sigma^{\prime}\left(z^{l}\right)$

   - 使用梯度下降训练参数

     $w^{l} \rightarrow w^{l}-\frac{\eta}{m} \sum_{x} \delta^{x, l}\left(a^{x, l-1}\right)^{T}$
     $b^{l} \rightarrow b^{l}-\frac{\eta}{m} \sum_{x} \delta^{x, l}$
   
   <a href="#fc_code">具体代码实现</a>

### 5. 卷积神经网络的推理与反向传播

CNN源码：[CNN source-code](https://github.com/lyp2333/CNN.git)<a name='cnn_code'> </a>

> 卷积神经网络反向传播过程与全连接网络类似，均是先将误差项$\delta$传到每一层，再求出每一层权值的梯度完成反向传播。

我们考虑步长为1、输入的深度为1、filter个数为1的最简单的情况。假设输入的大小为3*3，filter大小为2*2，按步长为1卷积，我们将得到2*2的**feature map**。

用$\delta_{i,j}^{l-1}$表示第$l$层第$i$行第$j$列的**误差项**；用$w_{m,n}$表示filter第$m$行第$n$列权重，用$w_b$表示filter的**偏置项**；用$a_{i,j}^{l-1}$表示第$l-1$层第$i$行第$j$列神经元的**输出**；用$net_{i,j^{l-1}}$表示第$l-1$行神经元的**加权输入**；用$\delta_{i,j}^{l-1}$表示第$l$层第$i$行第$j$列的**误差项**；用$f^{l-1}$表示第$l-1$层的**激活函数**。它们之间的关系如下:
$$
\begin{align} net^l&=conv(W^l, a^{l-1})+w_b\\ a^{l-1}_{i,j}&=f^{l-1}(net^{l-1}_{i,j}) \end{align}
$$
上式中，$net^l$、$W^l$、$a^{l-1}$都是数组，$W^l$是由$w^{m,n}$组成的数组，$conv$表示卷积操作。根据链式法则：
$$
\begin{align}
\delta^{l-1}_{i,j}&=\frac{\partial{E_d}}{\partial{net^{l-1}_{i,j}}}\\
&=\frac{\partial{E_d}}{\partial{a^{l-1}_{i,j}}}\frac{\partial{a^{l-1}_{i,j}}}{\partial{net^{l-1}_{i,j}}}
\end{align}
$$
我们先求第一项$\frac{\partial{E_d}}{\partial{a^{l-1}_{i,j}}}$,我们先来看几个特例，然后从中总结出一般性的规律。
$$
\because  
net^j_{1,1}=w_{1,1}a^{l-1}_{1,1}+w_{1,2}a^{l-1}_{1,2}+w_{2,1}a^{l-1}_{2,1}+w_{2,2}a^{l-1}_{2,2}+w_b\\
\therefore
\begin{align}
\frac{\partial{E_d}}{\partial{a^{l-1}_{1,1}}}&=\frac{\partial{E_d}}{\partial{net^{l}_{1,1}}}\frac{\partial{net^{l}_{1,1}}}{\partial{a^{l-1}_{1,1}}}\\
&=\delta^l_{1,1}w_{1,1}
\end{align}
$$
以此类推，我们求得:
$$
net^j_{1,1}=w_{1,1}a^{l-1}_{1,1}+w_{1,2}a^{l-1}_{1,2}+w_{2,1}a^{l-1}_{2,1}+w_{2,2}a^{l-1}_{2,2}+w_b\\
net^j_{1,2}=w_{1,1}a^{l-1}_{1,2}+w_{1,2}a^{l-1}_{1,3}+w_{2,1}a^{l-1}_{2,2}+w_{2,2}a^{l-1}_{2,3}+w_b\\
\therefore
\begin{align}
\frac{\partial{E_d}}{\partial{a^{l-1}_{1,2}}}&=\frac{\partial{E_d}}{\partial{net^{l}_{1,1}}}\frac{\partial{net^{l}_{1,1}}}{\partial{a^{l-1}_{1,2}}}+\frac{\partial{E_d}}{\partial{net^{l}_{1,2}}}\frac{\partial{net^{l}_{1,2}}}{\partial{a^{l-1}_{1,2}}}\\
&=\delta^l_{1,1}w_{1,2}+\delta^l_{1,2}w_{1,1}
\end{align}
$$

$$
\begin{align}
\frac{\partial{E_d}}{\partial{a^{l-1}_{2,2}}}&=\frac{\partial{E_d}}{\partial{net^{l}_{1,1}}}\frac{\partial{net^{l}_{1,1}}}{\partial{a^{l-1}_{2,2}}}+\frac{\partial{E_d}}{\partial{net^{l}_{1,2}}}\frac{\partial{net^{l}_{1,2}}}{\partial{a^{l-1}_{2,2}}}+\frac{\partial{E_d}}{\partial{net^{l}_{2,1}}}\frac{\partial{net^{l}_{2,1}}}{\partial{a^{l-1}_{2,2}}}+\frac{\partial{E_d}}{\partial{net^{l}_{2,2}}}\frac{\partial{net^{l}_{2,2}}}{\partial{a^{l-1}_{2,2}}}\\
&=\delta^l_{1,1}w_{2,2}+\delta^l_{1,2}w_{2,1}+\delta^l_{2,1}w_{1,2}+\delta^l_{2,2}w_{1,1}
\end{align}
$$

相当于把第层的sensitive map周围补一圈0，在与180度翻转后的filter进行**cross-correlation**，就能得到想要结果.因此上图的计算可以用卷积公式完美的表达：
$$
\frac{\partial{E_d}}{\partial{a_l}}=\delta^l*W^l
$$
我们再求第二项$\frac{\partial{a^{l-1}_{i,j}}}{\partial{net^{l-1}_{i,j}}}$,仅求激活函数$f$的导数就行了:$\frac{\partial{a^{l-1}_{i,j}}}{\partial{net^{l-1}_{i,j}}}=f'(net^{l-1}_{i,j})$

将第一项和第二项组合起来，我们得到最终的公式：
$$
\begin{align}
\delta^{l-1}_{i,j}&=\frac{\partial{E_d}}{\partial{net^{l-1}_{i,j}}}\\
&=\frac{\partial{E_d}}{\partial{a^{l-1}_{i,j}}}\frac{\partial{a^{l-1}_{i,j}}}{\partial{net^{l-1}_{i,j}}}\\
&=\sum_m\sum_n{w^l_{m,n}\delta^l_{i+m,j+n}}f'(net^{l-1}_{i,j})\\
\delta^{l-1}&=\delta^l*W^l\circ f'(net^{l-1})
\end{align}
$$

1. 在进行输入层深度为D时的误差传递时

​				当输入深度为D时，filter的深度也必须为D，层的通道只与filter的通道的权重进行计算。因此，反向计算**误差项**时，用filter的通道权重对sensitivity map进行卷积，得到第层通道的sensitivity map。

2. filter数量为N时的误差传递

   filter数量为N时，输出层的深度也为N，第个filter卷积产生输出层的第个feature map。由于第层每个加权输入都同时影响了第层所有feature map的输出值，因此，反向计算误差项时，需要使用全导数公式。也就是，我们先使用第个filter对第层相应的第个sensitivity map进行卷积，得到一组N个层的偏sensitivity map。依次用每个filter做这种卷积，就得到D组偏sensitivity map。最后在各组之间将N个偏sensitivity map **按元素相加**，得到最终的N个层的sensitivity map：
   $$
   \delta^{l-1}=\sum_{d=0}^D\delta_d^l*W_d^l\circ f'(net^{l-1})\qquad(式9)
   $$

$a_{i,j}^l$是第$l-1$层的输出，$w_{i,j}$是第$l$层filter的权重，$\delta_{i,j}^l$是第$l$层的sensitivity map。我们的任务是计算$w_{i,j}$的梯度，即$\frac{\partial{E_d}}{\partial{w_{i,j}}}$

计算$\frac{\partial{E_d}}{\partial{w_{1,1}}}$:
$$
net^j_{1,1}=w_{1,1}a^{l-1}_{1,1}+w_{1,2}a^{l-1}_{1,2}+w_{2,1}a^{l-1}_{2,1}+w_{2,2}a^{l-1}_{2,2}+w_b\\
net^j_{1,2}=w_{1,1}a^{l-1}_{1,2}+w_{1,2}a^{l-1}_{1,3}+w_{2,1}a^{l-1}_{2,2}+w_{2,2}a^{l-1}_{2,3}+w_b\\
net^j_{2,1}=w_{1,1}a^{l-1}_{2,1}+w_{1,2}a^{l-1}_{2,2}+w_{2,1}a^{l-1}_{3,1}+w_{2,2}a^{l-1}_{3,2}+w_b\\
net^j_{2,2}=w_{1,1}a^{l-1}_{2,2}+w_{1,2}a^{l-1}_{2,3}+w_{2,1}a^{l-1}_{3,2}+w_{2,2}a^{l-1}_{3,3}+w_b
$$
从上面的公式看出,根据全导数公式，计算$\frac{\partial{E_d}}{\partial{w_{1,1}}}$就是要把每个偏导数都加起来：

$$\begin{align} \frac{\partial{E_d}}{\partial{w_{1,1}}}&=\frac{\partial{E_d}}{\partial{net^{l}_{1,1}}}\frac{\partial{net^{l}_{1,1}}}{\partial{w_{1,1}}}+\frac{\partial{E_d}}{\partial{net^{l}_{1,2}}}\frac{\partial{net^{l}_{1,2}}}{\partial{w_{1,1}}}+\frac{\partial{E_d}}{\partial{net^{l}_{2,1}}}\frac{\partial{net^{l}_{2,1}}}{\partial{w_{1,1}}}+\frac{\partial{E_d}}{\partial{net^{l}_{2,2}}}\frac{\partial{net^{l}_{2,2}}}{\partial{w_{1,1}}}\\ &=\delta^l_{1,1}a^{l-1}_{1,1}+\delta^l_{1,2}a^{l-1}_{1,2}+\delta^l_{2,1}a^{l-1}_{2,1}+\delta^l_{2,2}a^{l-1}_{2,2} \end{align}$$

实际上，每个**权重项**都是类似的，我们发现$\frac{\partial{E_d}}{\partial{w_{i,j}}}$计算规律是：$\frac{\partial{E_d}}{\partial{w_{i,j}}}=\sum_m\sum_n\delta_{m,n}a^{l-1}_{i+m,j+n}$,也就是用sensitivity map作为卷积核，在input上进行**cross-correlation**

偏置项的梯度实质上就是所有误差项之和
$$
\begin{align}
\frac{\partial{E_d}}{\partial{w_b}}&=\frac{\partial{E_d}}{\partial{net^{l}_{1,1}}}\frac{\partial{net^{l}_{1,1}}}{\partial{w_b}}+\frac{\partial{E_d}}{\partial{net^{l}_{1,2}}}\frac{\partial{net^{l}_{1,2}}}{\partial{w_b}}+\frac{\partial{E_d}}{\partial{net^{l}_{2,1}}}\frac{\partial{net^{l}_{2,1}}}{\partial{w_b}}+\frac{\partial{E_d}}{\partial{net^{l}_{2,2}}}\frac{\partial{net^{l}_{2,2}}}{\partial{w_b}}\\
&=\delta^l_{1,1}+\delta^l_{1,2}+\delta^l_{2,1}+\delta^l_{2,2}\\
&=\sum_i\sum_j\delta^l_{i,j}
\end{align}
$$
对于步长为S的卷积层，处理方法与传递**误差项**是一样的，首先将sensitivity map还原成步长为1时的sensitivity map，再用上面的方法进行计算。获得了所有的梯度之后，就是根据**B P算法**来更新每个权重。

Pooling层的训练

- 对于max pooling，下一层的**误差项**的值会原封不动的传递到上一层对应区块中的最大值所对应的神经元，而其他神经元的**误差项**的值都是0。
- 对于mean pooling，下一层的**误差项**的值会**平均分配**到上一层对应区块中的所有神经元。

<a href='#cnn_code'>具体代码实现</a>

### 6. 从传统视觉特征提取到卷积神经网络

1. 灰度化
   - 识别物体，最关键的因素是梯度（SIFT/HOG），梯度意味着边缘，这是最本质的部分，而计算梯度，自然就用到灰度图像了，可以把灰度理解为图像的强度。
   - 颜色，易受光照影响，难以提供关键信息，故将图像进行灰度化，同时也可以加快特征提取的速度。
2. 仿射不变性（平面上任意两条线，经过仿射变换后，仍保持原来的状态）
3. 具有可重复性、可区分性、准确性、有效性、鲁棒性的局部特征

接下来以Sift和Hog特征为例，介绍传统视觉特征提取

#### 6.1 SIFT特征提取方法

1. 构建DOG尺度空间：

   模拟图像数据的多尺度特征，大尺度抓住细节特征，小尺度注重概貌特征。通过构建高斯金字塔（每一层用不同的参数σ做高斯模糊（加权）），保证图像在任何尺度都能有对应的特征点，即保证**尺度不变性**。

2. 关键点搜索和定位：

   确定是否为关键点，需要将该点与同尺度空间不同σ值的图像中的相邻点比较，如果该点为max或min，则为一个特征点。找到所有特征点后，要去除低对比度和不稳定的边缘效应的点，留下具有代表性的关键点（比如，正方形旋转后变为菱形，如果用边缘做识别，4条边就完全不一样，就会错误；如果用角点识别，则稳定一些）。去除这些点的好处是**增强匹配的抗噪能力和稳定性**。最后，对离散的点做曲线拟合，得到精确的关键点的位置和尺度信息。

3. 方向赋值：

   为了实现**旋转不变性**，需要根据检测到的关键点的局部图像结构为特征点赋值。具体做法是用梯度方向直方图。在计算直方图时，每个加入直方图的采样点都使用圆形高斯函数进行加权处理，也就是进行高斯平滑。这主要是因为SIFT算法只考虑了尺度和旋转不变形，没有考虑**仿射不变性**。通过高斯平滑，可以使关键点附近的梯度幅值有较大权重，从而部分弥补没考虑仿射不变形产生的特征点不稳定。注意，一个关键点可能具有多个关键方向，这有利于增强图像匹配的鲁棒性。

4. 关键点描述子的生成：

   关键点描述子不但包括关键点，还包括关键点周围对其有贡献的像素点。这样可使关键点有更多的不变特性，提高目标匹配效率。在描述子采样区域时，需要考虑旋转后进行双线性插值，防止因旋转图像出现白点。同时，为了保证旋转不变性，要以特征点为中心，在附近领域内旋转θ角，然后计算采样区域的梯度直方图，形成n维SIFT特征矢量（如128-SIFT）。最后，为了去除光照变化的影响，需要对特征矢量进行归一化处理。

#### 6.2 HOG特征提取

1. 灰度化
2. 采用Gamma校正法对输入图像进行颜色空间的标准化（归一化），目的是调节图像的对比度，降低图像局部的阴影和光照变化所造成的影响，同时可以抑制噪音的干扰。
3. 计算图像每个像素的梯度（包括大小和方向），主要是为了捕获轮廓信息，同时进一步弱化光照的干扰
4. 将图像划分成小cells
5. 统计每个cell的梯度直方图
6. 将每几个cell组成一个block（例如3*3个cell/block），一个block内所有cell的特征descriptor串联起来便得到该block的HOG特征descriptor。
7. 将图像image内的所有block的HOG特征descriptor串联起来就可以得到该image的HOG特征描述子。这个就是最终的可供分类使用的特征向量了。

#### 6.3 SIFT和HOG的比较

- **共同点：** 都是基于图像中梯度方向直方图的特征提取方法
- **不同点：** 
  - SIFT 特征通常与使用SIFT检测器得到的兴趣点一起使用。这些兴趣点与一个特定的方向和尺度相关联。通常是在对一个图像中的方形区域通过相应的方向和尺度变换后，再计算该区域的SIFT特征。
  - HOG特征的单元大小较小，故可以保留一定的空间分辨率，同时归一化操作使该特征对局部对比度变化不敏感。
  - 结合SIFT和HOG方法，可以发现SIFT对于复杂环境下物体的特征提取具有良好的特性；而HOG对于刚性物体的特征提取具有良好的特性。

#### SIFT/HOG与神经网络特征提取的比较

> **随着深度学习的发展，通过神经网络提取特征得到了广泛的应用，那么，神经网络提取的特征与传统的SIFT/HOG等特征提取方法有什么不同呢？**

<img src="https://img-blog.csdnimg.cn/img_convert/fa799943802dfd6514b7bb5044830a6b.png" alt="img" style="zoom:50%;" />

这就是神经网络每层提取到的特征。由于是通过神经网络自动学习到了，因此也是无监督的特征学习过程（Unsupervised Feature Learning） 。直观上说，就是找到有意义的小patch再将其进行组合，就得到了上一层的feature，递归地向上学习。在不同object上训练，所得的边界偏移是非常相似的，但物体的各个部分和模型结构就会完全不同了。

 传统特征提取方法的研究过程和思路是非常有用的，因为这些方法具有较强的可解释性，它们对设计机器学习方法解决此类问题提供启发和类比。有部分人认为现有的卷积神经网络与这些特征提取方法有一定类似性，因为每个滤波权重实际上是一个线性的识别模式，与这些特征提取过程的边界与梯度检测类似。同时，池化的作用是统筹一个区域的信息，这与这些特征提取后进行的特征整合（如直方图等）类似。通过实验发现卷积网络开始几层实际上确实是在做边缘和梯度检测。
