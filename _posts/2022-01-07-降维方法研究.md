---
layout:     post
title:      "降维方法研究"
subtitle:   "Research on dimensionality reduction methods"
date:       2022-01-07 12:00:00
author:     "lyp"
header-img: "img/post-bg-infinity.jpg"
tags:
  - Pattern reg
---

<!-- 数学公式 -->

<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      inlineMath: [['$','$']]
    }
  });
</script>

## 摘要

数据维度的迅速膨胀会给计算带来指数级别难度的增长，因此，我们需要通过降维把无关或冗余的特征删掉。降维计算成了自然而然的想法。本文将降维方法分为特征抽取以及特征选择两大类，在特征抽取中又存在着线性降维和针对流形空间的非线性降维。近年来，深度学习技术的研究引起了学术界和工业界的广泛兴趣，本文同时将介绍Autoencoder这一其中的典型代表进行阐述，从而为后续视觉领域的发展提供思考。 

关键词**：**维度灾难；降维；计算机视觉；深度学习；Autoencoder

## 1 介绍

数据在原始高维空间中，可能包 含有冗余信息或噪音信息，从而造成误差， 降维可以减少冗余和降低噪音。降维可以寻找数据内部的本质特征。降维是一个非常具有挑战性的任务，尤其是当你不知道该从哪里开始的时候。数据量越大，分析结果越可信；但同时你真的会感到一片茫然，无从下手。面对这么多特征，在微观层面分析每个变量显然不可行，因为这至少要几天甚至几个月，而这背后的时间成本是难以估计的。为此，我们需要一种更好的方法来处理高维数据比如本文介绍的降维：一种能在减少数据集中特征数量的同时，避免丢失太多信息并保持改进模型性能的方法。同时针对不同的数据分布，降维的方法也不同，本文将着重分析不同降维方式的数学推导，从而使大家更加清晰可靠的利用这一工具。

## 2 特征选择

特征选择是指从全部特征中选取一个特征子集，使构造出来的模型更好。特征抽取后的新特征是原来特征的一个映射。特征选择的流程图如下：

<center><img src="C:\Users\冷易鹏\AppData\Roaming\Typora\typora-user-images\image-20211127145640417.png" alt="image-20211127145640417" style="zoom:150%;" /><!-- 数学公式 -->
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      inlineMath: [['$','$']]
    }
  });
</script>

<center>fig.1  特征选择流程图</center>

###  2.1 筛选器

筛选器通过分析特征子集内部的特点来衡量其好坏。这种方法主要有以下两种分类：相关性和互信息。

<center><img src="https://images2015.cnblogs.com/blog/911998/201604/911998-20160409095940125-1703460651.png" alt="img" style="zoom:70%;" /></center>

<center>fig.2  筛选器工作流程</center>

#### 2.1.1 相关性

<a name='footnote'> </a>相关性即分析特征之间的线性关系， 两个变量之间的相关系数越高，从一个变量去预测另一个变量的精确度就越高，这是因为相关系数越高，就意味着这两个变量的共变部分越多，所以从其中一个变量的变化就可越多地获知另一个变量的变化。如果两个变量之间的相关系数为1或-1，那么你完全可由变量$ x $去获知变量$ y $的值。

#### 2.1.2  互信息

前者针对线性关系特征可以很好的实现降维，但我们更多的面对的是非线性关系。<br>互信息是信息论里一种有用的信息度量，它可以看成是一个随机变量中包含的关于另一个随机变量的信息量，或者说是一个随机变量由于已知另一个随机变量而减少的不肯定性。而信息熵$ H(x) $可以很好的去刻画这一特性。
$$
H(x)=-\sum^{n}_{i=1}{p(x_i)log_2p(x_2)}
$$
根据链式规则，两个变量之间的信息熵$H(x,y)=H(x)+H(y|x) =H(y)+H(x|y)$,我们一般用信息差表示互信息，用$I(x;y) = H(x) -H(x|y)$来描述。<br>         任意两个变量$x$和$y$的互信息定义为$I(x,y)=\sum_{x\in X}{\sum_{y\in Y}{p(x,y)\log{_2}{\frac{p(x,y)}{p(x)p(y)}}}}$,为了把互信息量限制在$[0,1]$区间，需要把它除以每个独立变量的信息熵之和，然后就可以得到归一化后的互信息量$NI(X;Y)= \frac{I(X;Y)}{H(X)+H(Y)}$,通过这种方式，我们需要计算每一对特征的归一互信息量，对于具有较高互信息量的特征对，我们会把其中一个特征扔掉。从而实现降维。

### 2.2 封装器

封装器实际上是一个分类器，我们会用内置的函数选取子集对样本集进行分类，分类的精度用于衡量特征选取的标准。封装器的工作流程图如下：

<img src="https://images2015.cnblogs.com/blog/911998/201604/911998-20160409100412297-1322414587.png" alt="img" style="zoom: 50%;" />

<center>fig.3  封装器工作流程</center>

封装器中最为常见的方法是特征递归消除，一种将预期的特征数量作为参数，在特征子集中进行循环迭代求解的一种方法。我们可以在sklearn库中调取使用。

## 3 特征抽取

### 3.1 线性方法

线性方法是针对于样本点处于欧式空间时所采用的方法，其中以PCA<sup><a href="#ref1"><font color='black'>[1]</font></a></sup>（主成分分析）和LDA<sup><a href="#ref1"><font color='black'>[2]</font></a></sup>（线性判别分析为代表）

#### 3.1.1 主成分分析

考虑二维空间一个二分类问题。希望沿着某 个轴进行投影得到的一维数据能区分两个类 。一维数据的决策点对应了原始数据的决策线。由于方差代表着在该投影方向上的信息量，我们便是要找到投影后使得方差最大的方向。

<img src="C:\Users\冷易鹏\AppData\Roaming\Typora\typora-user-images\image-20211127162030465.png" alt="image-20211127162030465" style="zoom:50%;" />

<center>fig.4  样本空间</center>

我们将问题转化为了一个最优化问题从而利用拉格朗日法进行求解:<br>        我们知道样本点$x_i$在方向$w$上的投影坐标为为：$(x_i,w)=x_i^Tw$,于是我们可以得到方差：
$$
\begin{aligned}
var(x) &= \frac{1}{m}\sum_{i=1}^{m}(x_i^Tw)^2\\
	   &= \frac{1}{m}\sum_{i=1}^{m}(x_i^Tw)^T(x_i^Tw)\\
	   &= \frac{1}{m}\sum_{i=1}^{m}{w^Tx_ix_i^Tw}\\
	   &= w^T(\frac1m\sum_{i=1}^{m}x_ix_i^T)w
\end{aligned}
$$
我们可以看到$\frac1m\sum_{i=1}^{m}x_ix_i^T$就是原样本的协方差，我们令该矩阵为$\wedge$,并构造拉格朗日函数，并对$w$求导
$$
\left\{\begin{array}{c}
\max \left\{w^{T} \Lambda w\right\} \\
\text { s.t. } w^{T} w=1
\end{array}\right.\\
L(w) =w^T\wedge w +\lambda(1-w^Tw)\\
\wedge w =\lambda w
$$

此时我们的方差为$var(x)=w^T\wedge w = \lambda w^Tw=\lambda$, $x$ 投影后的方差就是协方差矩阵的特征值。我们要找到最大方差也就是协方差矩阵最大的特征值，最佳投影方向就是最大特征值所对应的特征向量。

PCA降维的求解步骤如下，设有 h条 w 维数据。

1. 将原始数据进行转置
2. 将 X 的每一行进行零均值化，即减去这一行的均值；
3. 求出协方差矩阵，并求出其特征值和其特征向量
4. 重新自上而下排列顺序，$Y=PX$即为降维到k维的数据

#### 3.1.2 线性判别分析

线性判别分析法是一种有监督的线性降维方法，与PCA尽可能多地保留数据信息不同，LDA的目的是使降维后的数据点尽可能容易被区分。LDA试图寻找有大的类间差异和小的类内差异的投影轴；也就是最大化类间差异与类内差异的比值。假设共有C类。所有数据样本的中心是$\mu$。 第i类的中心是$\mu$，第$i$类数据点个数为$N_i$， $P_i=\frac{N_i}{N}$, 第$i$类的零均值数据矩阵是$X_i$，同时满足 $\sum_i=\frac{X_iX_i^T}{N_i}$

<img src="C:\Users\冷易鹏\AppData\Roaming\Typora\typora-user-images\image-20211127212549595.png" alt="image-20211127212549595" style="zoom:80%;" />

<center>fig.5  LDA的样本空间</center>

现在我们考虑给定一个方向$u$(投影轴)，计算类内方差和内间方差，则平均的类内散度矩阵（对称阵）和类间散度矩阵为
$$
W = \sum_{i}P_i \sum_i\\
B= \sum_{i}P_i(\mu_i-\mu)(\mu_i-\mu)^T
$$
给定方向$u$(单位向量)，投影后类内均方差和类间方差为$u^TW u$和$u^TB u$,因此优化目标变为
$$
\begin{aligned}
\max J &= \frac{u^TWu}{u^TBu}\\
	   &=u^TW^{-1}Bu
\end{aligned}
$$
现在我们的优化目标变得和PCA很像了，可以把$W^{-1}B$看作PCA中的$\sum$，但是PCA中$\sum$是对称阵，因此直接利用$W^{-1}B$求解会导致不是旋转变换，这样投影轴不是正交的。同时$W$中可能存在0，这样$W$是不可逆的，因此我们要丢掉$W$中的0特征值和负特征值，以及那些 很小的特征值。

最终LDA算法的求解过程如下：

1. 特征分解$W=U\wedge U^T$
2. 丢掉$\wedge$中的0特征值、负特征值、小特征值 ，以及对应的U中的特征向量。
3. 计算矩阵${B}^{\prime}={W}^{-1} {B}={U} \boldsymbol{\Lambda}^{-{1}} {U}^{\top}{B}$
4. 特征分解$B'=V\theta V^T$,LDA变换矩阵为$V$
5. 新的数据矩阵为$X'=V^TX$

​		

### 3.2 非线性方法

非线性方法又称流行学习方法，假设数据是均匀采样于一个高维欧氏空间中的低维流形，流形学习就是从高维[采样数据](https://baike.baidu.com/item/采样数据/7656238)中恢复低维流形结构，即找到高维空间中的低维流形，并求出相应的嵌入映射，以实现维数约简或者数据可视化。它是从观测到的现象中去寻找事物的本质，找到产生数据的内在规律。目前大火的人工智能可解释性也正在朝着流形学习的方向的发展。

<img src="https://images2018.cnblogs.com/blog/1335117/201807/1335117-20180715165410034-423842635.png" alt="img" style="zoom:80%;" />

<center>fig. 6 流形空间</center>

#### 3.2.1  核化线性降维

KPCA<sup><a href='ref1'><font color='black'>[3]</font></a></sup>是一种非线性映射的方法，核主成分分析是对PCA的一种推广。KPCA主要利用了核函数，即对于当前非线性不可分数据，将其映射至更高维的空间至线性可分，再进行降维，而其中利用核函数可求得内积，进而得到样本在特征向量上的投影。<br> 		现在考虑映射函数$\phi(\cdot):x_i\to\phi(x_i)$,将原本空间的样本点$x_i$ 映射到新空间中的点$\phi(\cdot)$;这种映射可视为从低维空间向高维空间映射 ，在低维空间里不可分的数据，在高维空间 容易变得可分。在新空间里数据矩阵变为$[\phi(x_i),\cdots,\phi(x_n)]$,对应的矩阵$K$变为：$K_{ij}=\phi(x_i)^T\phi(x_j)$。在PCA中，我们数据矩阵均是0均值化的，因此$K$也要满足中心化这个条件。设$K'$为中心化后的矩阵。
$$
\left.
\begin{aligned}
\left(\mathbf{K}^{\prime}\right)_{i j}&=\left(\phi_{i}-\frac{1}{n} \sum_{k} \phi_{k}\right)^{T}\left(\phi_{j}-\frac{1}{n} \sum_{k} \phi_{k}\right) \\
&=\phi_{i}^{T} \phi_{j}-\frac{1}{n} \sum_{k} \phi_{i}^{T} \phi_{k}-\frac{1}{n} \sum_{k} \phi_{k}^{T} \phi_{j}-\frac{1}{n^{2}} \sum_{k} \phi_{k}^{T} \sum_{k} \phi_{k} \\
&=K_{i j}-\frac{1}{n} \sum_{k} K_{i k}-\frac{1}{n} \sum_{k} K_{k j}-\frac{1}{n^{2}} \sum_{k} \sum_{l} K_{k l}
\end{aligned}
\right.\\
\mathbf{K}^{\prime}=\mathbf{K}-\mathbf{K} \mathbf{1}_{n}-\mathbf{1}_{n}^{T} \mathbf{K}-\mathbf{1}_{n}^{T} \mathbf{K} \mathbf{1}_{n}
$$
对$K’$进行特征值分解并将其坐标转化核分析形式可得
$$
K'=\hat{U}\wedge \hat{U}^T\\
\mathbf{\Lambda}_{[1: d]}^{-\frac{1}{2}} \mathbf{U}_{1: \mathrm{d}}^{T}\left[\begin{array}{c}
\mathbf{x}_{1}^{T} \mathbf{x} \\
\vdots \\
\mathbf{x}_{n}^{T} \mathbf{x}
\end{array}\right]
\to
\hat{\boldsymbol{\Lambda}}_{[1: d]}^{-\frac{1}{2}} \hat{\mathbf{U}}_{1: \mathrm{d}}^{T}\left[\begin{array}{c}
\phi\left(\mathbf{x}_{1}\right)^{T} \phi(\mathbf{x}) \\
\vdots \\
\phi\left(\mathbf{x}_{n}\right)^{T} \phi(\mathbf{x})
\end{array}\right]
$$
对核函数我们知道，并需要知道具体的形式，只需知道$\phi(x_i)^T\phi(x_j)$的结果,因此，在核函数形式下，$x$的转换坐标和转换空间为
$$
\hat{\wedge}_{[1:d]}^{-\frac{1}{2}}\hat{U}_{[1:d]}^{T}
 \left[\begin{matrix}
 K(x_i,x)\\
 \vdots\\
 K(x_n,x)
 \end{matrix}\right]\\
 \hat{\wedge}_{[1:d]}^{-\frac{1}{2}}\hat{U}_{[1:d]}^{T}K'=
 \hat{\wedge}_{[1:d]}^{-\frac{1}{2}}\hat{U}_{[1:d]}^{T}
$$
针对不同的核函数对原始数据进行处理效果如下：

<img src="C:\Users\冷易鹏\AppData\Roaming\Typora\typora-user-images\image-20211127224138314.png" alt="image-20211127224138314" style="zoom:80%;" />

<center>fig.7  不同的核函数的效果，中间是多项式核，右边是高斯核</center>

#### 3.2.2  局部线性嵌入

LLE<sup><a href='ref1'><font color='black'>[4]</font></a></sup>是一种流行学习方法，他认为在流行空间的局部空间内可以将以看作欧氏空间。LLE试图保 持邻域内样本之间的线性关系，核心思想是每一个数据点都可以由其近邻点的线性组合构造得到。<br> 		LLE的推导部分如下：<br>        设原空间中的$x_i$在低维空间中变为$z_i$<br> 	   设$x_i$的近邻样本集合为$N(X_i)$,计算出用$N(X_i)$的样本线性组合出$x_i$的组合系数$w_i$,我们将其转化为一个最小化问题：
$$
\min_{w_1,w_2,\cdots,w_m}\sum_{i=1}^{m}\|x_i-\sum_{j\in N(x_i)}w_{ij}x_j\|\\
\text{s.t.} \sum_{j\in N(x_i)}w_{ij}=1
$$
可求出：
$$
w_{i j}=\frac{\sum_{k \in {N}\left(\mathbf{x}_{i}\right)} C_{j k}^{-1}}
{\sum_{l, s \in {N}\left(\mathbf{x}_{i}\right)} C_{l s}^{-1}}\\
\text{其中} C_{jk}=(x_i-x_j)^T(x_i-x_k)
$$
将其映射到低维空间时$w_{ij}$保持不变，因此将上述最小化问题转化为了低维空间的优化任务：
$$
\min_{z_1,z_2,\cdots,z_m}\sum_{i=1}^{m}\|z_i-\sum_{j\in N(x_i)}w_{ij}z_j\|\\
\text{s.t.} \sum_{j\in N(x_i)}w_{ij}=1\\
\downarrow\\
\min_{z} tr(ZMZ^T)\\
s.t. ZZ^T = I\\
其中M=(I-W)^T(I-W)
$$
我们对$M$进行特征分解，$M=U\wedge U^T$,其中最小的几个特征值对应的特征向量组成的矩阵即为转化坐标。总结后算法流程如下：

1. 对于每个样本，确定邻域并计算出权重
2. 计算矩阵$M=(I-W)^T(I-W)$
3. M的最小的几个特征值对应的特征向量即为转化坐标

## 4 深度学习中的Auto-encoder

随着大数据的兴起，数据量在近些年来呈爆炸式增长。自监督学习可以充分利用数据构建更大，精度更高的模型。随着这种自监督学习的方式得以兴起。自编码器这个很久前的产物重新进入了大众的视野。<br> 	   自编码器是一种特殊的神经网络架构，他的输入和输出是架构是相同的。自编码器是通过无监督的方式来训练来获取输入数据在较低维度的表达。在神经网络的后段这些低纬度的信息表达再被重构回高维的数据表达。一个自编码器可以理解成是一个回归任务，用来预测他的输入。这些神经网络一般中层都会像下面的图中有很窄的层，这与降维的思想不谋而合，强迫神经网络来构建很高效但又相比浅层更高效的数据表达将前馈中的表达数据压缩以供后续的解码器重构原始的输入。<br>

<center><img src="C:\Users\冷易鹏\AppData\Roaming\Typora\typora-user-images\image-20211127235158757.png" alt="image-20211127235158757" style="zoom:50%;" /></center> 	

<center>fig.8  经典自编码机</center>

### 4.1  Denoising AutoEncoder

其思想很朴素，相较与经典自编码机<sup><a href='ref1'><font color='black'>[5,6]</font></a></sup>仅改动了一点：将输入数据加噪声或直接遮挡后送进模型，然后期望恢复没有噪声的原始图像。通过这样的方法，我们认为能提取到更好更鲁棒性的表征。从流形学习的角度来讲，我们期望算法可以将讲加了噪声的数据重新还原，这样模型也就有能力对区分出更有效的特征，同时使得模型具有更好的有更好的鲁棒性。

<img src="C:\Users\冷易鹏\AppData\Roaming\Typora\typora-user-images\image-20211128001231689.png" alt="image-20211128001231689" style="zoom: 67%;" />

<center>fig.9  从流形学习的角度去理解</center>

### 4.2  差分自编码机

VAE<sup><a href='ref1'><font color='black'>[6]</font></a></sup>相对较新，是一种生成模型，常用于生成图片。核心优势是其生成图片可控性好，缺点是生成的图像较为模糊，内容不清晰，相比之下GAN能生成更清晰的图像。其核心架构仍是Encoder-Decoder，但在中间加了模糊化的环节。具体来看，给定一个真实样本$X_k$，我们假设存在其后验分布$P(Z|X_i)$，并进一步假设这个分布是正态分布。从各个正态分布采样，再通过生成器来生成样本和真实样本对比。通过loss的设计，在这个过程中，使得采样变量成为更加具有鲁棒性的特征。而这个特征可以用来作为重建目标。

<img src="https://spaces.ac.cn/usr/uploads/2018/03/2584918486.png" alt="img" style="zoom: 50%;" />

<center>  fig.10  VAE 结构图</center>



## 5  总结

本文主要讨论了常见的线性与非线性降维方法并进行了数学推导，同时针对深度学习这一领域中与降维关联极大的自编码机进行了讨论。旨在为后续研究者给予一些启发。

## 参考文献<span name = "ref1"> </span>

<br>[1]  Jolliffe I. Principal component analysis[J]. Encyclopedia of statistics in behavioral science, 2005.<br>[2]  Martinez A M, Kak A C. Pca versus lda[J]. IEEE transactions on pattern analysis and machine intelligence, 2001, 23(2): 228-233.<br>[3]  Schölkopf B, Smola A, Müller K R. Nonlinear component analysis as a kernel eigenvalue problem[J]. Neural computation, 1998, 10(5): 1299-1319.<br>[4]  Roweis S T, Saul L K. Nonlinear dimensionality reduction by locally linear embedding[J]. science, 2000, 290(5500): 2323-2326.<br>[5]  He K, Chen X, Xie S, et al. Masked Autoencoders Are Scalable Vision Learners [Z]. 2021: arXiv:2111.06377<br>[6]  Ackley D H, Hinton G E, Sejnowski T J. A learning algorithm for Boltzmann machines[J]. Cognitive science, 1985, 9(1): 147-169.<br>[7]  Bao H, Dong L, Wei F. BEiT: BERT Pre-Training of Image Transformers [Z]. 2021: arXiv:2106.08254<br>



